# -*- coding: utf-8 -*-
"""Spam_Email_Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vmq_KGLUx2IK09YxO8Gxl-sKDlLla-GN
"""

import pandas as pd
import numpy as np
from google.colab import files
import os
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from bs4 import BeautifulSoup
import joblib
from imblearn.over_sampling import SMOTE
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
from nltk.corpus import stopwords
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB
from sklearn.svm import SVC
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score

# data = files.upload()

df = pd.read_csv("Email_Data.csv")

email_content_15th_row = df.at[1, 'text']

# Print the email content for the 15th row
print(email_content_15th_row)

# steps to be performed

# Data and Text Cleaning
# Tokenization
# Stemming or Lemmatization
# Handling Imbalanced Data
# Vectorization
# Normalization or Standardization (if needed)

df = df.drop_duplicates()
df = df.dropna()

# functions to calculate words, sentences, and characters
def count_words(text):
    words = re.findall(r'\b\w+\b', text)
    return len(words)

def count_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    return len(sentences)

def count_characters(text):
    return len(text)

def count_spaces(text):
    spaces = text.count(' ')
    return spaces

# Apply the custom functions to create new columns
df['word_count'] = df['text'].apply(count_words)
df['sentence_count'] = df['text'].apply(count_sentences)
df['character_count'] = df['text'].apply(count_characters)
df['space_count'] = df['text'].apply(count_spaces)

# Display the dataset with the new columns
print("Dataset with Word, Sentence, Character, and Space Counts:")
print(df.head())

df.head()

spam_df = df[df['label']==1]
ham_df = df[df['label']==0]

spam_df.describe()

ham_df.describe()

spam_count = df[df['label'] == 1].shape[0]
ham_count = df[df['label'] == 0].shape[0]

# Create a bar graph
labels = ['Spam', 'Ham']
counts = [spam_count, ham_count]

plt.bar(labels, counts, color=['red', 'green'])
plt.title('Distribution of Spam and Ham Emails')
plt.xlabel('Email Type')
plt.ylabel('Count')
plt.show()

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation and non-alphabetic characters
    text = ''.join([char for char in text if char.isalpha() or char.isspace()])

    # Tokenization
    tokens = word_tokenize(text)

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    return tokens

# Apply the preprocessing function to the 'text' column
df['preprocessed_tokens'] = df['text'].apply(preprocess_text)

# Display the DataFrame with the preprocessed tokens column
print("DataFrame with Preprocessed Tokens:")
print(df[['text', 'preprocessed_tokens']].head())

df.head()

df_subset = df.head(5000)

spam_dft = df_subset[df_subset['label']==1]
spam_dft.describe()
# ham_dft = df_subset[df_subset['label']==0]
# ham_dft.describe()

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Assuming 'preprocessed_tokens' column contains your preprocessed tokens
X = df_subset['preprocessed_tokens']
y = df_subset['label']  # Assuming 'label' column contains the target labels

# Convert preprocessed_tokens to strings
X_str = X.apply(lambda tokens: ' '.join(tokens))

# Use CountVectorizer to transform the text data to a dense array
cv = CountVectorizer()
X_vectorized = cv.fit_transform(X_str).toarray()

# Check the type of X_vectorized
print(type(X_vectorized))

# Rest of your code for model training and evaluation...

X_vectorized.shape
print(X_vectorized)

X_train, X_test , y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2 , random_state = 2)

vectorizers = [
    ('CountVectorizer', CountVectorizer()),
    ('TfidfVectorizer', TfidfVectorizer())
]

for vectorizer_name, vectorizer in vectorizers:
    print(f'\nUsing {vectorizer_name}:')

    # Naive Bayes models
    models = [
        ('MultinomialNB', MultinomialNB()),
        ('BernoulliNB', BernoulliNB()),
        ('GaussianNB', GaussianNB())
    ]

    for model_name, model in models:
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions on the test set
        y_pred = model.predict(X_test)

        # Evaluate the model
        print(f'{model_name}')

        print("Accuracy: ", accuracy_score(y_test,y_pred))
        print("Confusion Matrix")
        print(confusion_matrix(y_test,y_pred))
        print("Precision: ", precision_score(y_test,y_pred))
        print("\n\n")

# Multinomial Naive Bayes model
model_name = 'MultinomialNB'
model = MultinomialNB()

# Define parameter grid for GridSearchCV
param_grid = {'alpha': [0.1, 0.5, 1.0]}

# Create GridSearchCV object
grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=5)

# Fit the model with vectorized data
vectorizer = CountVectorizer()
grid_search.fit(X_train, y_train)

# Make predictions on the test set
y_pred = grid_search.predict(X_test)

# Print evaluation metrics
print(f'{model_name} Evaluation:')
print("Best Parameters:", grid_search.best_params_)
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Precision: ", precision_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Assume user_input is the input provided by the user
user_input = input("Enter the mail: ")

# Preprocess the user input
preprocessed_input = preprocess_text(user_input)

# Convert preprocessed input to a string
user_input_str = ' '.join(preprocessed_input)

# Use CountVectorizer to transform the user input to a dense array
user_input_vectorized = cv.transform([user_input_str]).toarray()


prediction = grid_search.predict(user_input_vectorized)

if prediction == 1:
  print("SPAM EMAIL")

else:
  print("HAM EMAIL")